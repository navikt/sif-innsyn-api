apiVersion: nais.io/v1
kind: Alert
metadata:
  name: {{app}}-alerts
  labels:
    team: {{team}}
  namespace: {{namespace}}
spec:
  receivers:
    slack:
      channel: {{slack-channel}}
      prependText: "{{{slack-notify-type}}}"
  alerts:
    - alert: Applikasjon nede
      severity: danger
      expr: up{app="{{app}}", job="kubernetes-pods"} == 0
      for: 2m
      description: "App \{{ $labels.app }} er nede i namespace \{{ $labels.kubernetes_namespace }}"
      action: "`kubectl describe pod \{{ $labels.kubernetes_pod_name }} -n \{{ $labels.kubernetes_namespace }} -c \{{ $labels.app }}` for events, og `kubectl logs \{{ $labels.kubernetes_pod_name }} -n \{{ $labels.kubernetes_namespace }} -c \{{ $labels.app }}` for logger"

    - alert: høy andel warning i logger
      severity: warning
      expr: (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="{{app}}",log_level=~"Warning"}[3m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="{{app}}"}[3m]))) > 100
      for: 3m
      action: "Sjekk loggene til app \{{ $labels.log_app }} i namespace \{{ $labels.log_namespace }}, for å se hvorfor det er så mye warnings"

    - alert: høy andel error i logger
      severity: danger
      expr: (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="{{app}}",log_level=~"Error"}[3m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="{{app}}"}[3m]))) > 0
      for: 3m
      action: "Sjekk loggene til app \{{ $labels.log_app }} i namespace \{{ $labels.log_namespace }}, for å se hvorfor det er så mye feil"

    - alert: Høy andel HTTP serverfeil (5xx responser)
      severity: danger
      expr: floor(increase(http_server_requests_seconds_count{status=~"5.*", app="{{app}}"}[3m])) > 1
      for: 1m
      description: "Følgende request feilet: `Status \{{ $labels.status }} - \{{ $labels.method }} \{{ $labels.route }}`.\n
                    Grunn:\n ```\{{ $labels.problem_details }}```\n
                    Sjekk loggene for å se hvorfor dette feiler."
      action: "`kubectl logs \{{ $labels.kubernetes_pod_name }} -n \{{ $labels.kubernetes_namespace }} -c \{{ $labels.app }}`"

    - alert: Høy andel HTTP klientfeil (4xx responser)
      severity: danger
      expr: floor(increase(http_server_requests_seconds_count{status=~"4.*", status!~"404|401|403", app="{{app}}"}[3m])) > 0
      for: 1m
      description: "Følgende request feilet: `Status \{{ $labels.status }} - \{{ $labels.method }} \{{ $labels.route }}`.\n
                    Grunn:\n ```\{{ $labels.problem_details }}```\n
                    Sjekk loggene for å se hvorfor dette feiler"
      action: "`kubectl logs \{{ $labels.kubernetes_pod_name }} -n \{{ $labels.kubernetes_namespace }} -c \{{ $labels.app }}`"

    - alert: Konsumering av meldinger feiler
      severity: danger
      expr: ceil(increase(spring_kafka_listener_seconds_count{result="failure", app="{{app}}"}[3m])) > 0
      for: 1m
      description: "Konsumering av melding i \{{ $labels.name }} feiler. Sjekk loggene for å finne ut hvorfor dette feiler."
      action: "`kubectl logs \{{ $labels.pod_name }} -n \{{ $labels.namespace }} -c \{{ $labels.app }}`"

    - alert: Helsesjekk feiler
      expr: floor(increase(http_server_requests_seconds_count{status!~"200", uri="/actuator/health", app="{{app}}"}[3m])) > 0
      severity: warning
      for: 2m
      desription: "Sjekk loggene for å se hvorfor helsesjekken feiler.`"
      action: "`kubectl logs \{{ $labels.kubernetes_pod_name }} -n \{{ $labels.kubernetes_namespace }} -c \{{ $labels.app }}`"
